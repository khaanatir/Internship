{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8ac606-5fcc-4ded-8015-85328a5be6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "\n",
    "# Set up the Chrome driver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Get the novel name from the user\n",
    "novel_name = input(\"Enter the novel name: \")\n",
    "\n",
    "# Navigate to Amazon India\n",
    "driver.get(\"https://www.amazon.in/\")\n",
    "\n",
    "# Enter the novel name in the search box\n",
    "search_box = driver.find_element(By.XPATH, \"//input[@id='twotabsearchtextbox']\")\n",
    "search_box.send_keys(novel_name)\n",
    "\n",
    "# Submit the search form\n",
    "search_box.submit()\n",
    "\n",
    "df = pd.DataFrame(columns=[\"Brand Name\", \"Product Name\", \"Price\", \"Return/Exchange\", \"Expected Delivery\", \"Availability\", \"Product URL\"])\n",
    "\n",
    "for page in range(1, 4):\n",
    "    search_results = WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_all_elements_located((By.XPATH, \"//div[@data-component-type='s-search-result']\"))\n",
    "    )\n",
    "    \n",
    "    for result in search_results:\n",
    "        try:\n",
    "            brand_name = result.find_element(By.XPATH, \".//span[@class='a-size-base-plus a-color-base']\").text\n",
    "        except:\n",
    "            brand_name = \"null\"\n",
    "        \n",
    "        try:\n",
    "            product_name = result.find_element(By.XPATH, \".//span[@class='a-size-base-plus a-color-base a-text-normal']\").text\n",
    "        except:\n",
    "            product_name = \"null\"\n",
    "        \n",
    "        try:\n",
    "            price = result.find_element(By.XPATH, \".//span[@class='a-price-whole']\").text\n",
    "        except:\n",
    "            price = \"null\"\n",
    "        \n",
    "        try:\n",
    "            return_exchange = result.find_element(By.XPATH, \".//span[@class='a-size-base a-color-base a-text-normal']\").text\n",
    "        except:\n",
    "            return_exchange = \"null\"\n",
    "        \n",
    "        try:\n",
    "            expected_delivery = result.find_element(By.XPATH, \".//span[contains(text(), 'Get it by')]\").text\n",
    "        except:\n",
    "            expected_delivery = \"null\"\n",
    "        \n",
    "        try:\n",
    "            availability = result.find_element(By.XPATH, \".//span[@aria-label='Currently unavailable']\").text\n",
    "        except:\n",
    "            availability = \"null\"\n",
    "        \n",
    "        try:\n",
    "            product_url = result.find_element(By.XPATH, \".//a[@class='a-link-normal a-text-normal']\").get_attribute(\"href\")\n",
    "        except:\n",
    "            product_url = \"null\"\n",
    "        \n",
    "        df = df.append({\"Brand Name\": brand_name, \"Product Name\": product_name, \"Price\": price, \n",
    "                        \"Return/Exchange\": return_exchange, \"Expected Delivery\": expected_delivery, \n",
    "                        \"Availability\": availability, \"Product URL\": product_url}, ignore_index=True)\n",
    "    \n",
    "    try:\n",
    "        next_page = driver.find_element(By.XPATH, \"//li[@class='a-last']/a\")\n",
    "        next_page.click()\n",
    "    except:\n",
    "        break\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# Display the dataframe\n",
    "print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8bc5c0-26ba-4c45-8b2b-bab0eb54b495",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "\n",
    "# Set up the WebDriver\n",
    "driver = webdriver.Edge()\n",
    "\n",
    "# Keywords to search for\n",
    "keywords = ['fruits', 'cars', 'Machine Learning', 'Guitar', 'Cakes']\n",
    "\n",
    "# store the images\n",
    "image_dir = 'images'\n",
    "if not os.path.exists(image_dir):\n",
    "    os.makedirs(image_dir)\n",
    "\n",
    "# Loop through each keyword\n",
    "for keyword in keywords:\n",
    "    driver.get(\"https://images.google.com/\")\n",
    "\n",
    "    search_bar = driver.find_element(By.NAME, \"q\")\n",
    "    search_bar.send_keys(keyword)\n",
    "    search_bar.send_keys(Keys.RETURN)\n",
    "\n",
    "    time.sleep(2)\n",
    "\n",
    "    image_elements = driver.find_elements(By.CSS_SELECTOR, \"img.rg_i\")\n",
    "    image_urls = []\n",
    "\n",
    "    for image_element in image_elements[:10]:\n",
    "        image_url = image_element.get_attribute(\"src\")\n",
    "        image_urls.append(image_url)\n",
    "\n",
    "    # Save the images\n",
    "    for i, image_url in enumerate(image_urls):\n",
    "        driver.get(image_url)\n",
    "        filename = f\"{keyword}_{i+1}.jpg\"\n",
    "        with open(os.path.join(image_dir, filename), \"wb\") as f:\n",
    "            f.write(driver.find_element(By.TAG_NAME, \"body\").screenshot_as_png)\n",
    "        print(f\"Saved image {filename}\")\n",
    "\n",
    "    print(f\"Done searching for {keyword}\")\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cd162f-d340-429e-9a20-f9cfb22fa084",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "\n",
    "# Function to fetch details from Flipkart search results using Selenium\n",
    "def fetch_flipkart_smartphone_details(search_query):\n",
    "    url = \"https://www.flipkart.com/\"\n",
    "    \n",
    "    # Set up Chrome WebDriver\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(url)\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        close_login_popup = driver.find_element(By.XPATH, '//button[@class=\"_2KpZ6l _2doB4z\"]')\n",
    "        close_login_popup.click()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    search_box = driver.find_element(By.XPATH, '//input[@title=\"Search for products, brands and more\"]')\n",
    "    search_box.send_keys(search_query)\n",
    "    search_box.submit()\n",
    "    \n",
    "    time.sleep(3)  \n",
    "    \n",
    "    smartphone_details = []\n",
    "    \n",
    "    results = driver.find_elements(By.XPATH, '//div[@class=\"_1AtVbE\"]/div')\n",
    "    \n",
    "    for result in results:\n",
    "        try:\n",
    "            product_url = result.find_element(By.XPATH, './/a[@class=\"s1Q9rs\"]').get_attribute('href')\n",
    "            brand_name = result.find_element(By.XPATH, './/div[@class=\"_4rR01T\"]').text.split()[0]\n",
    "            smartphone_name = result.find_element(By.XPATH, './/a[@class=\"IRpwTa\"]').text\n",
    "            color = result.find_element(By.XPATH, './/a[@class=\"IRpwTa\"]/following-sibling::div').text\n",
    "            ram = result.find_element(By.XPATH, './/ul/li[1]').text.split('|')[0].strip()\n",
    "            rom = result.find_element(By.XPATH, './/ul/li[1]').text.split('|')[1].strip()\n",
    "            primary_camera = result.find_element(By.XPATH, './/ul/li[3]').text\n",
    "            secondary_camera = result.find_element(By.XPATH, './/ul/li[4]').text\n",
    "            display_size = result.find_element(By.XPATH, './/ul/li[2]').text.split('(')[0].strip()\n",
    "            battery_capacity = result.find_element(By.XPATH, './/ul/li[5]').text\n",
    "            price = result.find_element(By.XPATH, './/div[@class=\"_30jeq3 _1_WHN1\"]').text\n",
    "            \n",
    "            details = {\n",
    "                \"Brand Name\": brand_name,\n",
    "                \"Smartphone Name\": smartphone_name,\n",
    "                \"Colour\": color,\n",
    "                \"RAM\": ram,\n",
    "                \"Storage(ROM)\": rom,\n",
    "                \"Primary Camera\": primary_camera,\n",
    "                \"Secondary Camera\": secondary_camera,\n",
    "                \"Display Size\": display_size,\n",
    "                \"Battery Capacity\": battery_capacity,\n",
    "                \"Price\": price,\n",
    "                \"Product URL\": product_url\n",
    "            }\n",
    "            \n",
    "            smartphone_details.append(details)\n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping product details: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    driver.quit()\n",
    "    \n",
    "    return smartphone_details\n",
    "\n",
    "# Main function to execute the scraping and save results to CSV\n",
    "def main():\n",
    "    search_query = \"Oneplus Nord\"  \n",
    "    smartphone_details = fetch_flipkart_smartphone_details(search_query)\n",
    "    \n",
    "    # Replace missing details with \"-\"\n",
    "    for details in smartphone_details:\n",
    "        for key in details:\n",
    "            if details[key] == \"\":\n",
    "                details[key] = \"-\"\n",
    "    \n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame(smartphone_details)\n",
    "    \n",
    "    # Save the DataFrame to CSV\n",
    "    csv_filename = f\"{search_query}_flipkart_smartphones.csv\"\n",
    "    df.to_csv(csv_filename, index=False)\n",
    "    \n",
    "    print(f\"Scraping and saving complete. Results saved in '{csv_filename}'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1a2867-ce7c-4e95-8003-79eeb0ffec66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import time\n",
    "import re\n",
    "def fetch_coordinates(city_name):\n",
    "    url = f\"https://www.google.com/maps/place/{city_name}\"\n",
    "    \n",
    "    # Set up Chrome WebDriver\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(url)\n",
    "    \n",
    "    time.sleep(3)  \n",
    "    \n",
    "     try:\n",
    "        # Find the coordinates element using XPath\n",
    "        coordinates_element = driver.find_element(By.XPATH, '//meta[@itemprop=\"geo\"]')\n",
    "        content = coordinates_element.get_attribute('content')\n",
    "        \n",
    "        # Parse the content attribute to extract latitude and longitude\n",
    "        coordinates = content.split(';')[0].split(',')\n",
    "        latitude = coordinates[0]\n",
    "        longitude = coordinates[1]\n",
    "        \n",
    "        print(f\"Coordinates for {city_name}: Latitude {latitude}, Longitude {longitude}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching coordinates: {str(e)}\")\n",
    "    \n",
    "    driver.quit()\n",
    "\n",
    "# Main function to execute the scraping\n",
    "def main():\n",
    "    city_name = \"New York\" \n",
    "    fetch_coordinates(city_name)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b1ad02-a2eb-4089-a0d0-19a091e51b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Set up the WebDriver (replace with your WebDriver executable path)\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Open the website digit.in\n",
    "driver.get('https://www.digit.in/top-products/best-gaming-laptops-40.html')\n",
    "\n",
    "time.sleep(5)  # Allow time for the page to load\n",
    "\n",
    "# Scrape the data for the best gaming laptops\n",
    "laptops = driver.find_elements(By.CLASS_NAME, 'TopNumbeHeading')\n",
    "\n",
    "laptop_details = []\n",
    "\n",
    "for laptop in laptops:\n",
    "    details = {}\n",
    "    details['Name'] = laptop.find_element(By.CLASS_NAME, 'TopNumbeHeading').text.strip()\n",
    "    details['Seller'] = laptop.find_element(By.CLASS_NAME, 'SellerName').text.strip()\n",
    "    details['Price'] = laptop.find_element(By.CLASS_NAME, 'Price').text.strip()\n",
    "    details['Specifications'] = laptop.find_element(By.CLASS_NAME, 'Specs').text.strip()\n",
    "    laptop_details.append(details)\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(laptop_details)\n",
    "print(df)\n",
    "\n",
    "# Quit the WebDriver\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c079df1a-9ee8-43cb-add5-7ba2a4e9d313",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "driver.get('https://www.forbes.com/billionaires/')\n",
    "\n",
    "time.sleep(5) \n",
    "\n",
    "# Initialize an empty list to store billionaire details\n",
    "billionaires_data = []\n",
    "\n",
    "last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "while True:\n",
    "    \n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "   \n",
    "    time.sleep(5)\n",
    "    \n",
    "    new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    if new_height == last_height:\n",
    "        break\n",
    "    last_height = new_height\n",
    "\n",
    "# Scrape the data for all billionaire entries\n",
    "billionaires = driver.find_elements(By.XPATH, \"//div[@class='table-row-group__item']\")\n",
    "\n",
    "for billionaire in billionaires:\n",
    "    try:\n",
    "        rank = billionaire.find_element(By.XPATH, \".//div[contains(@class, 'rank')]\").text.strip()\n",
    "        name = billionaire.find_element(By.XPATH, \".//div[contains(@class, 'personName')]\").text.strip()\n",
    "        net_worth = billionaire.find_element(By.XPATH, \".//div[contains(@class, 'netWorth')]\").text.strip()\n",
    "        age = billionaire.find_element(By.XPATH, \".//div[contains(@class, 'age')]\").text.strip()\n",
    "        citizenship = billionaire.find_element(By.XPATH, \".//div[contains(@class, 'countryOfCitizenship')]\").text.strip()\n",
    "        source = billionaire.find_element(By.XPATH, \".//div[contains(@class, 'source')]\").text.strip()\n",
    "        industry = billionaire.find_element(By.XPATH, \".//div[contains(@class, 'category')]\").text.strip()\n",
    "\n",
    "        billionaires_data.append({\n",
    "            'Rank': rank,\n",
    "            'Name': name,\n",
    "            'Net worth': net_worth,\n",
    "            'Age': age,\n",
    "            'Citizenship': citizenship,\n",
    "            'Source': source,\n",
    "            'Industry': industry\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping data for a billionaire: {e}\")\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(billionaires_data)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "csv_filename = 'forbes_billionaires.csv'\n",
    "df.to_csv(csv_filename, index=False)\n",
    "\n",
    "print(f\"Scraping and saving complete. Results saved in '{csv_filename}'\")\n",
    "\n",
    "# Quit the WebDriver\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003d1948-7b36-439e-89b1-7d7c317bf97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Function to extract comments, upvotes, and timestamps\n",
    "def extract_comments(video_url, min_comments=500):\n",
    "    # Set up the WebDriver (replace with your WebDriver executable path)\n",
    "    driver = webdriver.Chrome()\n",
    "\n",
    "    # Open the YouTube video URL\n",
    "    driver.get(video_url)\n",
    "    \n",
    "    # Scroll down to load comments\n",
    "    time.sleep(5)  # Allow time for the page to load\n",
    "    \n",
    "    #scroll to load more comments\n",
    "    last_height = driver.execute_script(\"return document.documentElement.scrollHeight\")\n",
    "    while True:\n",
    "        driver.execute_script(\"window.scrollTo(0, document.documentElement.scrollHeight);\")\n",
    "        time.sleep(5)\n",
    "        new_height = driver.execute_script(\"return document.documentElement.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "\n",
    "    comments_data = []\n",
    "    \n",
    "    # Extract comments, upvotes, and timestamps\n",
    "    comments = driver.find_elements(By.XPATH, \"//ytd-comment-thread-renderer\")\n",
    "    for comment in comments[:min_comments]:\n",
    "        try:\n",
    "            comment_text = comment.find_element(By.XPATH, \".//yt-formatted-string[@id='content-text']\").text.strip()\n",
    "            try:\n",
    "                upvotes = comment.find_element(By.XPATH, \".//span[@id='vote-count-middle']\").text.strip()\n",
    "            except:\n",
    "                upvotes = '0'\n",
    "            timestamp = comment.find_element(By.XPATH, \".//a[@class='yt-simple-endpoint style-scope ytd-comment-renderer']\").text.strip()\n",
    "            \n",
    "            comments_data.append({\n",
    "                'Comment': comment_text,\n",
    "                'Upvotes': upvotes,\n",
    "                'Timestamp': timestamp\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting comment data: {e}\")\n",
    "\n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame(comments_data)\n",
    "    \n",
    "    # Save the DataFrame to a CSV file\n",
    "    csv_filename = 'youtube_comments.csv'\n",
    "    df.to_csv(csv_filename, index=False)\n",
    "    \n",
    "    print(f\"Scraping and saving complete. Results saved in '{csv_filename}'\")\n",
    "\n",
    "    # Quit the WebDriver\n",
    "    driver.quit()\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    video_url = 'https://www.youtube.com/watch?v=LCsmBH8F0l4'  # Replace with the video URL\n",
    "    extract_comments(video_url)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a77ff53-43f4-48c5-8d07-3631496cea91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Function to extract hostel data\n",
    "def extract_hostel_data():\n",
    "    # Set up the WebDriver (replace with your WebDriver executable path)\n",
    "    driver = webdriver.Chrome()\n",
    "\n",
    "    # Open the Hostelworld London page\n",
    "    driver.get('https://www.hostelworld.com/s?q=London&city=London&country=England')\n",
    "\n",
    "    time.sleep(5)  # Allow time for the page to load\n",
    "\n",
    "    hostels_data = []\n",
    "\n",
    "    while True:\n",
    "        hostels = driver.find_elements(By.XPATH, \"//div[@class='fabresult-content']\")\n",
    "        \n",
    "        for hostel in hostels:\n",
    "            try:\n",
    "                name = hostel.find_element(By.XPATH, \".//h2/a\").text.strip()\n",
    "                distance = hostel.find_element(By.XPATH, \".//span[@class='distance-description']\").text.strip()\n",
    "                ratings = hostel.find_element(By.XPATH, \".//div[@class='score orange big']\").text.strip()\n",
    "                total_reviews = hostel.find_element(By.XPATH, \".//div[@class='left-col']/div[@class='review']\").text.strip()\n",
    "                overall_reviews = hostel.find_element(By.XPATH, \".//div[@class='left-col']/div[@class='keyword']\").text.strip()\n",
    "                privates_from_price = hostel.find_element(By.XPATH, \".//div[@class='price-col']/a[1]\").text.strip()\n",
    "                dorms_from_price = hostel.find_element(By.XPATH, \".//div[@class='price-col']/a[2]\").text.strip()\n",
    "                facilities = hostel.find_element(By.XPATH, \".//div[@class='rating-facilities']\").text.strip()\n",
    "                property_desc = hostel.find_element(By.XPATH, \".//div[@class='description']\").text.strip()\n",
    "                \n",
    "                hostels_data.append({\n",
    "                    'Name': name,\n",
    "                    'Distance from City Centre': distance,\n",
    "                    'Ratings': ratings,\n",
    "                    'Total Reviews': total_reviews,\n",
    "                    'Overall Reviews': overall_reviews,\n",
    "                    'Privates from Price': privates_from_price,\n",
    "                    'Dorms from Price': dorms_from_price,\n",
    "                    'Facilities': facilities,\n",
    "                    'Property Description': property_desc\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Error extracting hostel data: {e}\")\n",
    "\n",
    "        # Check if there is a next page button\n",
    "        try:\n",
    "            next_button = driver.find_element(By.XPATH, \"//a[@class='pagination-next']\")\n",
    "            next_button.click()\n",
    "            time.sleep(5)  # Allow time for the next page to load\n",
    "        except Exception:\n",
    "            break\n",
    "\n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame(hostels_data)\n",
    "    \n",
    "    # Save the DataFrame to a CSV file\n",
    "    csv_filename = 'hostelworld_london_hostels.csv'\n",
    "    df.to_csv(csv_filename, index=False)\n",
    "    \n",
    "    print(f\"Scraping and saving complete. Results saved in '{csv_filename}'\")\n",
    "\n",
    "    # Quit the WebDriver\n",
    "    driver.quit()\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    extract_hostel_data()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ffa285-e001-4ff2-a9aa-b4186fa09f4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
