{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eaaba9e3-1391-4981-a3ec-e82eee13f3e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\rihaa\\anaconda3\\lib\\site-packages (4.22.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in c:\\users\\rihaa\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (2.0.7)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\rihaa\\anaconda3\\lib\\site-packages (from selenium) (0.26.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\rihaa\\anaconda3\\lib\\site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\rihaa\\anaconda3\\lib\\site-packages (from selenium) (2024.2.2)\n",
      "Requirement already satisfied: typing_extensions>=4.9.0 in c:\\users\\rihaa\\anaconda3\\lib\\site-packages (from selenium) (4.9.0)\n",
      "Requirement already satisfied: websocket-client>=1.8.0 in c:\\users\\rihaa\\anaconda3\\lib\\site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\rihaa\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (23.2.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\rihaa\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\rihaa\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.4)\n",
      "Requirement already satisfied: outcome in c:\\users\\rihaa\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\rihaa\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\rihaa\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.16.0)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\rihaa\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\rihaa\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\rihaa\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\rihaa\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1668e49f-d04e-4b1a-bf45-27523c1cf400",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 1\n",
    "\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.webdriver.common.by import By\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e26205ac-d0a8-42e8-8f05-391043ac5b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "db580c3b-b276-47a9-a5a1-e0ec5dc1187e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "#Open the website naukri.com\n",
    "driver.get('https://www.naukri.com/')\n",
    "\n",
    "# Entering the designation and location\n",
    "designation = driver.find_element(By.CLASS_NAME,\"suggestor-input\")\n",
    "designation.send_keys('Data Analyst')\n",
    "location = driver.find_element(By.XPATH,\"/html/body/div[1]/div[6]/div/div/div[5]/div/div/div/div[1]/div/input\")\n",
    "location.send_keys('Delhi/NCR')\n",
    "\n",
    "# Click the search button\n",
    "\n",
    "search = driver.find_element(By.CLASS_NAME,'qsbSubmit')\n",
    "search.click()\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "#entering salary filter\n",
    "salary_filter = driver.find_element(By.XPATH, \"//span[text()='3-6 Lakhs']\")\n",
    "salary_filter.click()\n",
    "\n",
    "# Scraping the data fpr first 10 jobs\n",
    "\n",
    "jobs = driver.find_elements(By.XPATH, \"//article[@class='jobTuple bgWhite br4 mb-8']\")\n",
    "job_data = []\n",
    "\n",
    "for job in jobs[:10]: \n",
    "    # Scraping first 10 job results\n",
    "    job_title = job.find_element(By.CLASS_NAME, 'title.fw500.ellipsis').text\n",
    "    company_name = job.find_element(By.CLASS_NAME, 'subTitle.ellipsis.fleft').text\n",
    "    job_location = job.find_element(By.CLASS_NAME, 'fleft.grey-text.br2.placeHolderLi.location').text\n",
    "    experience_required = job.find_element(By.CLASS_NAME, 'fleft.grey-text.br2.placeHolderLi.experience').text\n",
    "    \n",
    "    job_data.append({\n",
    "        'Job Title': job_title,\n",
    "        'Company Name': company_name,\n",
    "        'Job Location': job_location,\n",
    "        'Experience Required': experience_required\n",
    "    })\n",
    "\n",
    "# Creating a DataFrame\n",
    "df = pd.DataFrame(job_data)\n",
    "\n",
    "# Displaying the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b789df-5d20-4eff-90ca-5cc30229037a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 2\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Set up the WebDriver\n",
    "driver = webdriver.Edge()\n",
    "\n",
    "# Open the website shine.com\n",
    "driver.get('https://www.shine.com/')\n",
    "\n",
    "jobs = driver.find_element(By.XPATH,'/html/body/div/ul/li[2]/a/span')\n",
    "jobs.click()\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "#ENTER JOB TITLE AND LOCATION\n",
    "\n",
    "job_title = driver.find_element(By.XPATH, \"/html/body/div/div[2]/div/div/div[1]/div[1]/div/div[2]/div/div/form/div/div[1]/ul/li[1]/div/input\")\n",
    "job_title.send_keys('Data Analyst')\n",
    "location = driver.find_element(By.XPATH, '/html/body/div/div[2]/div/div/div[1]/div[1]/div/div[2]/div/div/form/div/div[1]/ul/li[2]/div/input')\n",
    "location.send_keys('Bangalore')\n",
    "\n",
    "#Click Search\n",
    "search_button = driver.find_element(By.XPATH, \"/html/body/div/div[2]/div/div/div[1]/div[1]/div/div[2]/div/div[1]/form/div/div[2]/div/button\")\n",
    "search_button.click()\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "# Scrape the data for the first 10 job results\n",
    "jobs = driver.find_elements(By.XPATH, \"//li[@class='result-display__block']\")\n",
    "job_data = []\n",
    "\n",
    "for job in jobs[:10]:\n",
    "    job_title = job.find_element(By.CLASS_NAME, 'result-display__title').text\n",
    "    company_name = job.find_element(By.CLASS_NAME, 'result-display__profile').text\n",
    "    job_location = job.find_element(By.CLASS_NAME, 'result-display__location').text\n",
    "    experience_required = job.find_element(By.CLASS_NAME, 'result-display__experience').text\n",
    "    \n",
    "    job_data.append({\n",
    "        'Job Title': job_title,\n",
    "        'Company Name': company_name,\n",
    "        'Job Location': job_location,\n",
    "        'Experience Required': experience_required\n",
    "    })\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(job_data)\n",
    "print(df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1ccd7525-9a74-4c53-bdf7-c131548d6f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "#Question 3\n",
    "\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "\n",
    "driver= webdriver.Edge()\n",
    "\n",
    "driver.get('https://www.flipkart.com/apple-iphone-11-black-64-gb/product-reviews/itm4e5041ba101fd?pid=MOBFWQ6BXGJCEYNY&lid=LSTMOBFWQ6BXGJCEYNYZXSHRJ&marketplace=FLIPKART')\n",
    "\n",
    "review_data = []\n",
    "\n",
    "# going through the reviews\n",
    "for i in range(100):\n",
    "    # Get the review container\n",
    "    reviews = driver.find_elements(By.XPATH, \"//div[@class='t-ZTKy']\")\n",
    "    if i < len(reviews):\n",
    "        review = reviews[i]\n",
    "        \n",
    "        # Extract the rating\n",
    "        rating = review.find_element(By.XPATH, \".//div[@class '_3LWZlK _1BLPMq']\").text\n",
    "        \n",
    "        # Extract the review summary\n",
    "        summary = review.find_element(By.XPATH, \".//p[@class='_2-N8zT']\").text\n",
    "        \n",
    "        # Extract the full review\n",
    "        full_review = review.find_element(By.XPATH, \".//div[@class='qwjRop']\").text\n",
    "        \n",
    "        # Add the review data to the list\n",
    "        review_data.append({\n",
    "            'Rating': rating,\n",
    "            'Review Summary': summary,\n",
    "            'Full Review': full_review\n",
    "        })\n",
    "    \n",
    "    # Scroll down to load more reviews\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(2)\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(review_data)\n",
    "\n",
    "print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fe5f88-6a33-4d40-86fa-e776aedcf36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "\n",
    "# Set up the webdriver\n",
    "driver = webdriver.Chrome()\n",
    "driver.get('https://www.flipkart.com/')\n",
    "\n",
    "time.sleep(10)\n",
    "\n",
    "#Search for sneakers\n",
    "\n",
    "search = driver.find_element(By.CLASS_NAME,'Pke_EE')\n",
    "search.send_keys('sneakers')\n",
    "\n",
    "time.sleep(10)\n",
    "\n",
    "#clicking on search button\n",
    "search_button = driver.find_element(By.CLASS_NAME,'_2iLD__')\n",
    "search_button.click()\n",
    "\n",
    "brand = []\n",
    "product_description = []\n",
    "price = []\n",
    "\n",
    "#scraping brand from the first 100 sneakers\n",
    "for i in range(40):  \n",
    "    brand_tags = driver.find_elements(By.CLASS_NAME, '_2WkVRV')\n",
    "    description_tags = driver.find_elements(By.CLASS_NAME, 'IRpwTa')\n",
    "    price_tags = driver.find_elements(By.CLASS_NAME, '_30jeq3')\n",
    "\n",
    "    for j in range(len(brand_tags)):\n",
    "        if len(brand) >= 100:\n",
    "            break\n",
    "        try:\n",
    "            brand.append(brand_tags[j].text)\n",
    "            product_description.append(description_tags[j].text)\n",
    "            price.append(price_tags[j].text)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    if len(brand) >= 100:\n",
    "        break\n",
    "\n",
    "    # Click on the next page button\n",
    "    try:\n",
    "        next_button = driver.find_element(By.XPATH, \"//a[contains(text(), 'Next')]\")\n",
    "        next_button.click()\n",
    "        time.sleep(5)\n",
    "    except:\n",
    "        print(\"No more pages available.\")\n",
    "        break\n",
    "\n",
    "# Creating a DataFrame with the scraped data\n",
    "sneakers_df = pd.DataFrame({\n",
    "    'Brand': brand[:100],\n",
    "    'Product Description': product_description[:100],\n",
    "    'Price': price[:100]\n",
    "})              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "3f590851-cedc-4b2c-850f-4420e3cd37ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# opening Amazon.in\n",
    "driver.get(\"https://www.amazon.in/\")\n",
    "time.sleep(5)\n",
    "\n",
    "# Input \"Laptop\" into the search box and submit\n",
    "search_box = driver.find_element(By.XPATH,'/html/body/div[1]/header/div/div[1]/div[2]/div/form/div[2]/div[1]/input')\n",
    "search_box.send_keys(\"Laptop\")\n",
    "search_box.send_keys(Keys.RETURN)\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "#Setting CPU type\n",
    "\n",
    "cpu_i7 = driver.find_element(By.XPATH, \"//span[text()='Intel Core i7']\")\n",
    "cpu_i7.click()\n",
    "\n",
    "# Scrape the first 10 laptops' data\n",
    "laptops = []\n",
    "\n",
    "# Locate all laptop containers on the first page\n",
    "laptop_containers = driver.find_elements(By.XPATH, \"//div[@data-component-type='s-search-result']\")[:10]\n",
    "\n",
    "for container in laptop_containers:\n",
    "    try:\n",
    "        # Scrape the title\n",
    "        title = container.find_element(By.XPATH, \".//span[@class='a-size-medium a-color-base a-text-normal']\").text\n",
    "    except:\n",
    "        title = \"No title available\"\n",
    "    \n",
    "    try:\n",
    "        # Scrape the ratings\n",
    "        ratings = container.find_element(By.XPATH, \".//span[@class='a-icon-alt']\").text\n",
    "    except:\n",
    "        ratings = \"No ratings available\"\n",
    "    \n",
    "    try:\n",
    "        # Scrape the price\n",
    "        price = container.find_element(By.XPATH, \".//span[@class='a-price-whole']\").text\n",
    "    except:\n",
    "        price = \"No price available\"\n",
    "    \n",
    "    laptops.append({\"Title\": title, \"Ratings\": ratings, \"Price\": price})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "ab2245ea-0918-4518-bdac-74c970565930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Open the webpage\n",
    "driver.get(\"https://www.azquotes.com/\")\n",
    "time.sleep(5)\n",
    "\n",
    "# Click on the Top Quotes link\n",
    "top_quotes_link = driver.find_element(By.XPATH, \"//a[text()='Top Quotes']\")\n",
    "top_quotes_link.click()\n",
    "\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "# Initialize a list \n",
    "quotes_data = []\n",
    "\n",
    "while len(quotes_data) < 1000:\n",
    "    # Locate all quote containers on the current page\n",
    "    quote_containers = driver.find_elements(By.XPATH, \"//div[@class='wrap block grid-item qb clearfix']\")\n",
    "\n",
    "    # Scrape data from each container\n",
    "    for container in quote_containers:\n",
    "        if len(quotes_data) >= 1000:\n",
    "            break\n",
    "        try:\n",
    "            # Scrape the quote\n",
    "            quote = container.find_element(By.XPATH, \".//a[@title='view quote']\").text\n",
    "        except:\n",
    "            quote = \"No quote available\"\n",
    "\n",
    "        try:\n",
    "            # Scrape the author\n",
    "            author = container.find_element(By.XPATH, \".//div[@class='author']\").text\n",
    "        except:\n",
    "            author = \"No author available\"\n",
    "\n",
    "        try:\n",
    "            # Scrape the type of quote\n",
    "            quote_type = container.find_element(By.XPATH, \".//div[@class='tags']\").text\n",
    "        except:\n",
    "            quote_type = \"No type available\"\n",
    "\n",
    "        quotes_data.append({\"Quote\": quote, \"Author\": author, \"Type\": quote_type})\n",
    "\n",
    "    # Click on the next page link if available\n",
    "    try:\n",
    "        next_page_link = driver.find_element(By.XPATH, \"//a[@class='pager-next']\")\n",
    "        next_page_link.click()\n",
    "        time.sleep(5)\n",
    "    except:\n",
    "        break\n",
    "\n",
    "# Close the browser session\n",
    "driver.quit()\n",
    "\n",
    "# Create a DataFrame and print the results\n",
    "df = pd.DataFrame(quotes_data)\n",
    "print(df.head(10))  # Print the first 10 quotes for verification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "0f707c39-61d8-4269-b815-219357c39e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# OpenING the URL\n",
    "driver.get(\"https://www.jagranjosh.com/general-knowledge/list-of-all-prime-ministers-of-india-1473165149-1\")\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "# Initialize a list to hold Prime Minister data\n",
    "pm_data = []\n",
    "\n",
    "# Locate all rows containing PM data\n",
    "pm_rows = driver.find_elements(By.XPATH, \"//table[contains(@class, 'table-box')]//tbody//tr\")\n",
    "\n",
    "# Scrape data from each row\n",
    "for row in pm_rows:\n",
    "    try:\n",
    "        # Scrape the name\n",
    "        name = row.find_element(By.XPATH, \"./td[1]\").text\n",
    "    except:\n",
    "        name = \"No name available\"\n",
    "    \n",
    "    try:\n",
    "        # Scrape the Born-Dead\n",
    "        born_dead = row.find_element(By.XPATH, \"./td[2]\").text\n",
    "    except:\n",
    "        born_dead = \"No Born-Dead available\"\n",
    "    \n",
    "    try:\n",
    "        # Scrape the Term of office\n",
    "        term = row.find_element(By.XPATH, \"./td[3]\").text\n",
    "    except:\n",
    "        term = \"No Term of office available\"\n",
    "    \n",
    "    try:\n",
    "        # Scrape the Remarks\n",
    "        remarks = row.find_element(By.XPATH, \"./td[4]\").text\n",
    "    except:\n",
    "        remarks = \"No Remarks available\"\n",
    "    \n",
    "    pm_data.append({\"Name\": name, \"Born-Dead\": born_dead, \"Term of office\": term, \"Remarks\": remarks})\n",
    "\n",
    "\n",
    "df = pd.DataFrame(pm_data)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "b9301c1e-27db-4a6d-b423-94d8c2116181",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "expected ':' (4152165433.py, line 34)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[142], line 34\u001b[1;36m\u001b[0m\n\u001b[1;33m    for i in name_tags\u001b[0m\n\u001b[1;37m                      ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m expected ':'\n"
     ]
    }
   ],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Open the URL\n",
    "driver.get(\"https://www.motor1.com/\")\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "# Searching 50 most expensive cars\n",
    "search_box = driver.find_element(By.XPATH, \"/html/body/div[9]/div[2]/div/div/div[3]/div/div/div/form/input\")\n",
    "search_box.send_keys(\"50 most expensive cars\")\n",
    "search_box.send_keys(Keys.RETURN)\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "#CLICKING ON  ONE OF THE LINK\n",
    "article_link = driver.find_element(By.XPATH, \"/html/body/div[9]/div[9]/div/div[1]/div/div/div[1]/div/div[1]/h3/a\")\n",
    "article_link.click()\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "car_name = []\n",
    "car_price = []\n",
    "name_tags = driver.find_elements(By.CLASS_NAME,\"subheader\")\n",
    "for i in name_tags\n",
    "name = i.text\n",
    "car_name.append(name)\n",
    "\n",
    "price_tag = driver.find_elements(By.XPATH,'/html/body/div[9]/div[7]/div[2]/div[1]/div[2]/div[2]/p[4]')\n",
    "for i in  price_tags\n",
    "price =i.text\n",
    "car_price.append(price)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f1b29c-8607-405c-8e19-cae0ffa1e28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Open the URL\n",
    "driver.get(\"https://www.motor1.com/\")\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "# Searching 50 most expensive cars\n",
    "search_box = driver.find_element(By.XPATH, \"/html/body/div[9]/div[2]/div/div/div[3]/div/div/div/form/input\")\n",
    "search_box.send_keys(\"50 most expensive cars\")\n",
    "search_box.send_keys(Keys.RETURN)\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "# Clicking on one of the links\n",
    "article_link = driver.find_element(By.XPATH, \"/html/body/div[9]/div[9]/div/div[1]/div/div/div[1]/div/div[1]/h3/a\")\n",
    "article_link.click()\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "car_name = []\n",
    "car_price = []\n",
    "\n",
    "# Collecting car names\n",
    "name_tags = driver.find_elements(By.CLASS_NAME, \"subheader\")\n",
    "for i in name_tags:\n",
    "    name = i.text\n",
    "    car_name.append(name)\n",
    "\n",
    "# Collecting car prices\n",
    "price_tags = driver.find_elements(By.XPATH, '/html/body/div[9]/div[7]/div[2]/div[1]/div[2]/div[2]/p[4]')\n",
    "for i in price_tags:\n",
    "    price = i.text\n",
    "    car_price.append(price)\n",
    "\n",
    "\n",
    "# Create a DataFrame and print the results\n",
    "df = pd.DataFrame({\"Car name\": car_name, \"Price\": car_price})\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c5edec-8421-43b5-b57b-c72488be44db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
